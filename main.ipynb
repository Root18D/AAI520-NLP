{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\code\\myvenv\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\code\\myvenv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: gradio in d:\\code\\myvenv\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: filelock in d:\\code\\myvenv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\code\\myvenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\code\\myvenv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\code\\myvenv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\code\\myvenv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (4.6.0)\n",
      "Requirement already satisfied: fastapi<1.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.115.0)\n",
      "Requirement already satisfied: ffmpy in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (1.4.0)\n",
      "Requirement already satisfied: httpx>=0.24.1 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (3.10.7)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: ruff>=0.2.2 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio) (0.31.0)\n",
      "Requirement already satisfied: fsspec in d:\\code\\myvenv\\lib\\site-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in d:\\code\\myvenv\\lib\\site-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\code\\myvenv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\code\\myvenv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\code\\myvenv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in d:\\code\\myvenv\\lib\\site-packages (from fastapi<1.0->gradio) (0.38.6)\n",
      "Requirement already satisfied: certifi in d:\\code\\myvenv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\code\\myvenv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\code\\myvenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\code\\myvenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\code\\myvenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\code\\myvenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\code\\myvenv\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in d:\\code\\myvenv\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: colorama in d:\\code\\myvenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\code\\myvenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\code\\myvenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\code\\myvenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\code\\myvenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\code\\myvenv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\code\\myvenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\code\\myvenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\code\\myvenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\code\\myvenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_28692\\3200920126.py:91: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_28692\\3200920126.py:118: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\myvenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/5475 - Loss: 1.0645\n",
      "Batch 1000/5475 - Loss: 0.7119\n",
      "Batch 1500/5475 - Loss: 0.7037\n",
      "Batch 2000/5475 - Loss: 0.5603\n",
      "Batch 2500/5475 - Loss: 0.6453\n",
      "Batch 3000/5475 - Loss: 0.8732\n",
      "Batch 3500/5475 - Loss: 0.3840\n",
      "Batch 4000/5475 - Loss: 0.6124\n",
      "Batch 4500/5475 - Loss: 0.4306\n",
      "Batch 5000/5475 - Loss: 0.4597\n",
      "Training Loss: 1.5553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_28692\\3200920126.py:149: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1927\n",
      "\n",
      "Epoch 2/3\n",
      "Batch 500/5475 - Loss: 0.7118\n",
      "Batch 1000/5475 - Loss: 0.3461\n",
      "Batch 1500/5475 - Loss: 0.5781\n",
      "Batch 2000/5475 - Loss: 0.4724\n",
      "Batch 2500/5475 - Loss: 0.9673\n",
      "Batch 3000/5475 - Loss: 0.5110\n",
      "Batch 3500/5475 - Loss: 0.7702\n",
      "Batch 4000/5475 - Loss: 1.0179\n",
      "Batch 4500/5475 - Loss: 0.5730\n",
      "Batch 5000/5475 - Loss: 0.4679\n",
      "Training Loss: 1.0039\n",
      "Validation Loss: 1.1516\n",
      "\n",
      "Epoch 3/3\n",
      "Batch 500/5475 - Loss: 0.3241\n",
      "Batch 1000/5475 - Loss: 0.3788\n",
      "Batch 1500/5475 - Loss: 0.3265\n",
      "Batch 2000/5475 - Loss: 0.3812\n",
      "Batch 2500/5475 - Loss: 0.2204\n",
      "Batch 3000/5475 - Loss: 0.8653\n",
      "Batch 3500/5475 - Loss: 0.5740\n",
      "Batch 4000/5475 - Loss: 0.6528\n",
      "Batch 4500/5475 - Loss: 0.1769\n",
      "Batch 5000/5475 - Loss: 0.2374\n",
      "Training Loss: 0.8235\n",
      "Validation Loss: 1.1465\n",
      "Total Training Time: 4328.62 seconds\n",
      "Question: What did Albert Einstein develop?\n",
      "Answer: einsteinhaus, from 1903 to 1905, the year in which the annus mirabilis papers\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers scikit-learn gradio\n",
    "import json\n",
    "import time  \n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AdamW, BertForQuestionAnswering\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from torch.cuda.amp import GradScaler, autocast  # Automatic Mixed Precision\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# Load data function to reuse for both train and validation datasets\n",
    "def load_squad_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Load train and validation data\n",
    "train_texts, train_queries, train_answers = load_squad_data(Path(r\"D:\\Downloads\\archive (2)\\train-v1.1.json\"))\n",
    "val_texts, val_queries, val_answers = load_squad_data(Path(r\"D:\\Downloads\\archive (2)\\dev-v1.1.json\"))\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, val_queries, truncation=True, padding=True)\n",
    "\n",
    "# Function to calculate and add token positions\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        start_idx = answers[i]['answer_start']\n",
    "        end_idx = start_idx + len(answers[i]['text'])\n",
    "\n",
    "        start_positions.append(encodings.char_to_token(i, start_idx))\n",
    "        end_positions.append(encodings.char_to_token(i, end_idx - 1))\n",
    "\n",
    "        # Handle truncation cases where token positions are not found\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# Apply token positions function to both train and validation encodings\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create Dataset and DataLoader with pin_memory and larger batch size\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model setup\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Automatic Mixed Precision setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 3\n",
    "print_every = 500\n",
    "grad_accumulation_steps = 2  # Simulate larger batch sizes\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Automatic Mixed Precision (AMP)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss / grad_accumulation_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update model parameters after accumulating gradients\n",
    "        if (batch_idx + 1) % grad_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item() * grad_accumulation_steps\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "total_time = time.time() - whole_train_eval_time\n",
    "print(f\"Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"finetunedmodel.pt\")\n",
    "\n",
    "# TF-IDF Vectorizer for context retrieval\n",
    "vectorizer = TfidfVectorizer().fit(train_texts)\n",
    "\n",
    "# Function to retrieve the most relevant context\n",
    "def retrieve_relevant_context(question, contexts):\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    context_vectors = vectorizer.transform(contexts)\n",
    "    similarity_scores = (context_vectors * question_vector.T).toarray()\n",
    "    most_relevant_context_idx = similarity_scores.argmax()\n",
    "    return contexts[most_relevant_context_idx]\n",
    "\n",
    "# Function to perform inference\n",
    "def answer_question(context, question):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores) + 1\n",
    "\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = retrieve_relevant_context(\"What did Albert Einstein develop?\", train_texts)\n",
    "question = \"What did Albert Einstein develop?\"\n",
    "\n",
    "# Perform inference\n",
    "answer = answer_question(context, question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-finetuned-bert-model\\\\tokenizer_config.json',\n",
       " './my-finetuned-bert-model\\\\special_tokens_map.json',\n",
       " './my-finetuned-bert-model\\\\vocab.txt',\n",
       " './my-finetuned-bert-model\\\\added_tokens.json',\n",
       " './my-finetuned-bert-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./my-finetuned-bert-model\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./my-finetuned-bert-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('./my-finetuned-bert-model')\n",
    "model = BertForQuestionAnswering.from_pretrained('./my-finetuned-bert-model')\n",
    "\n",
    "# Move model to the appropriate device (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Function to perform inference\n",
    "def answer_question(context, question):\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores) + 1  # End index is inclusive\n",
    "\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gradio as gr\n",
    "\n",
    "# Load data function to reuse for context retrieval\n",
    "def load_squad_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Load train data for context retrieval\n",
    "train_texts, train_queries, train_answers = load_squad_data(Path(r\"D:\\Downloads\\archive (2)\\train-v1.1.json\"))\n",
    "\n",
    "# Load the fine-tuned model and tokenizer from the saved directory\n",
    "model = BertForQuestionAnswering.from_pretrained('./my-finetuned-bert-model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('./my-finetuned-bert-model')\n",
    "\n",
    "# Set the model to evaluation mode and move it to the appropriate device\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# TF-IDF Vectorizer for context retrieval\n",
    "vectorizer = TfidfVectorizer().fit(train_texts)\n",
    "\n",
    "# Function to retrieve the most relevant context\n",
    "def retrieve_relevant_context(question, contexts):\n",
    "    question_vector = vectorizer.transform([question])\n",
    "    context_vectors = vectorizer.transform(contexts)\n",
    "    similarity_scores = (context_vectors * question_vector.T).toarray()\n",
    "    most_relevant_context_idx = similarity_scores.argmax()\n",
    "    return contexts[most_relevant_context_idx]\n",
    "\n",
    "# Function to answer the user's question based on the retrieved context\n",
    "def answer_question(question):\n",
    "    # Retrieve the most relevant context based on the question\n",
    "    context = retrieve_relevant_context(question, train_texts)\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the most likely beginning and end of the answer\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores) + 1  # End index is inclusive\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Gradio interface function\n",
    "def qa_chatbot(question):\n",
    "    answer = answer_question(question)\n",
    "    return answer\n",
    "\n",
    "# Launch the Gradio chatbot interface\n",
    "interface = gr.Interface(\n",
    "    fn=qa_chatbot,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"BERT Q&A Chatbot\",\n",
    "    description=\"Ask any question, and the model will automatically retrieve the most relevant context and answer your question.\"\n",
    ")\n",
    "\n",
    "# Run the Gradio interface\n",
    "interface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
