{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\code\\myvenv\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in d:\\code\\myvenv\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\code\\myvenv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\code\\myvenv\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\code\\myvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\code\\myvenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in d:\\code\\myvenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\code\\myvenv\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\code\\myvenv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\code\\myvenv\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\code\\myvenv\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\code\\myvenv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_38544\\2465575039.py:90: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_38544\\2465575039.py:117: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\code\\myvenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/5475 - Loss: 1.4253\n",
      "Batch 1000/5475 - Loss: 0.5324\n",
      "Batch 1500/5475 - Loss: 0.6527\n",
      "Batch 2000/5475 - Loss: 1.0864\n",
      "Batch 2500/5475 - Loss: 0.8128\n",
      "Batch 3000/5475 - Loss: 0.6015\n",
      "Batch 3500/5475 - Loss: 0.3411\n",
      "Batch 4000/5475 - Loss: 0.7605\n",
      "Batch 4500/5475 - Loss: 0.7239\n",
      "Batch 5000/5475 - Loss: 0.5888\n",
      "Training Loss: 1.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_38544\\2465575039.py:148: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.1839\n",
      "\n",
      "Epoch 2/3\n",
      "Batch 500/5475 - Loss: 0.5269\n",
      "Batch 1000/5475 - Loss: 0.8885\n",
      "Batch 1500/5475 - Loss: 0.7134\n",
      "Batch 2000/5475 - Loss: 0.5960\n",
      "Batch 2500/5475 - Loss: 0.4270\n",
      "Batch 3000/5475 - Loss: 0.2735\n",
      "Batch 3500/5475 - Loss: 0.6517\n",
      "Batch 4000/5475 - Loss: 0.4618\n",
      "Batch 4500/5475 - Loss: 0.4078\n",
      "Batch 5000/5475 - Loss: 0.4785\n",
      "Training Loss: 0.9940\n",
      "Validation Loss: 1.1213\n",
      "\n",
      "Epoch 3/3\n",
      "Batch 500/5475 - Loss: 0.5703\n",
      "Batch 1000/5475 - Loss: 0.2953\n",
      "Batch 1500/5475 - Loss: 0.2911\n",
      "Batch 2000/5475 - Loss: 0.3003\n",
      "Batch 2500/5475 - Loss: 0.2909\n",
      "Batch 3000/5475 - Loss: 0.3565\n",
      "Batch 3500/5475 - Loss: 0.6252\n",
      "Batch 4000/5475 - Loss: 0.2483\n",
      "Batch 4500/5475 - Loss: 0.4970\n",
      "Batch 5000/5475 - Loss: 0.4957\n",
      "Training Loss: 0.8243\n",
      "Validation Loss: 1.1492\n",
      "Total Training Time: 4533.61 seconds\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from transformers import AutoTokenizer, AdamW, BertForQuestionAnswering\n",
    "from torch.cuda.amp import GradScaler, autocast  # For Automatic Mixed Precision\n",
    "\n",
    "# Load data function to reuse for both train and validation datasets\n",
    "def load_squad_data(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts, questions, answers = [], [], []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Load train and validation data once\n",
    "train_texts, train_queries, train_answers = load_squad_data(Path(r\"D:\\Downloads\\archive (2)\\train-v1.1.json\"))\n",
    "val_texts, val_queries, val_answers = load_squad_data(Path(r\"D:\\Downloads\\archive (2)\\dev-v1.1.json\"))\n",
    "\n",
    "# Tokenizer setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, val_queries, truncation=True, padding=True)\n",
    "\n",
    "# Function to calculate and add token positions\n",
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        start_idx = answers[i]['answer_start']\n",
    "        end_idx = start_idx + len(answers[i]['text'])  # Calculate end index using answer text length\n",
    "\n",
    "        start_positions.append(encodings.char_to_token(i, start_idx))\n",
    "        end_positions.append(encodings.char_to_token(i, end_idx - 1))  # End index should point to the last token\n",
    "\n",
    "        # Handle truncation cases where token positions are not found\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, end_idx - 2)  # Adjust for truncation\n",
    "            if end_positions[-1] is None:\n",
    "                end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# Now apply this function to both train and validation encodings\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)\n",
    "\n",
    "# Custom Dataset Class\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Create Dataset and DataLoader with pin_memory and larger batch size\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, pin_memory=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model setup\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Automatic Mixed Precision setup\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 3\n",
    "print_every = 500\n",
    "grad_accumulation_steps = 2  # Simulate larger batch sizes\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "whole_train_eval_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Automatic Mixed Precision (AMP)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss / grad_accumulation_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update model parameters after accumulating gradients\n",
    "        if (batch_idx + 1) % grad_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item() * grad_accumulation_steps\n",
    "\n",
    "        if (batch_idx + 1) % print_every == 0:\n",
    "            print(f\"Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "total_time = time.time() - whole_train_eval_time\n",
    "print(f\"Total Training Time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"finetunedmodel.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Board\\AppData\\Local\\Temp\\ipykernel_38544\\3516342945.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('finetunedmodel.pt'))  # Load the model state dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where is the Eiffel Tower located?\n",
      "Answer: paris, france\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertForQuestionAnswering\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "model.load_state_dict(torch.load('finetunedmodel.pt'))  # Load the model state dict\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Move model to the appropriate device (GPU/CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Function to perform inference\n",
    "def answer_question(context, question):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the most likely beginning and end of the answer\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "\n",
    "    # Find the tokens with the highest `start` and `end` scores\n",
    "    start_idx = torch.argmax(start_scores)\n",
    "    end_idx = torch.argmax(end_scores) + 1  # End index is inclusive\n",
    "\n",
    "    # Convert token indices back to words\n",
    "    answer_tokens = inputs['input_ids'][0][start_idx:end_idx]\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "\n",
    "# Example usage\n",
    "context = \"The Eiffel Tower is located in Paris, France. It was constructed in 1889.\"\n",
    "question = \"Where is the Eiffel Tower located?\"\n",
    "\n",
    "# Perform inference\n",
    "answer = answer_question(context, question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc06871371b343c4ad687afcb0703e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my-finetuned-bert-model\\\\tokenizer_config.json',\n",
       " './my-finetuned-bert-model\\\\special_tokens_map.json',\n",
       " './my-finetuned-bert-model\\\\vocab.txt',\n",
       " './my-finetuned-bert-model\\\\added_tokens.json',\n",
       " './my-finetuned-bert-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(\"./my-finetuned-bert-model\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"./my-finetuned-bert-model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of Building a BERT-based Question Answering Model\n",
    "This project focuses on fine-tuning a pre-trained BERT (Bidirectional Encoder Representations from Transformers) model for a Question Answering (QA) task using the SQuAD (Stanford Question Answering Dataset). BERT is a transformer-based model that has been pre-trained on large amounts of text data for tasks like masked language modeling and next-sentence prediction. Fine-tuning BERT for a downstream task like QA involves adapting the model’s weights to the specific domain of the task (in this case, answering questions based on passages of text).\n",
    "\n",
    "Step 1: Data Preprocessing\n",
    "The dataset used in this project is SQuAD, which contains passages (contexts), questions, and corresponding answers. The data is loaded from JSON files (train-v1.1.json and dev-v1.1.json). The first step involves reading this data and extracting the contexts, questions, and answers. These answers are marked with their starting positions (answer_start), and from this, we also calculate the ending position (answer_end) based on the length of the answer text.\n",
    "\n",
    "This step faces challenges such as ensuring that tokenization is consistent. BERT uses subword tokenization, meaning some words are split into smaller units. This can cause the start and end indices of the answers to shift, especially when long or unusual words are split. A critical task is to map the original character positions to token positions accurately.\n",
    "\n",
    "Step 2: Tokenization\n",
    "BERT requires inputs to be tokenized into subwords using the BERT tokenizer. The model is pre-trained with a specific tokenizer (WordPiece). Here, both the contexts (passages) and questions are tokenized. The tokenizer handles padding and truncation to ensure that each input sequence fits within the model’s maximum input length (usually 512 tokens for BERT). Special tokens like [CLS], which represents the start of a sequence, and [SEP], which separates questions from contexts, are automatically added by the tokenizer.\n",
    "\n",
    "A key challenge during tokenization is handling cases where the context or question exceeds the maximum length. While BERT can handle up to 512 tokens, contexts may sometimes be longer. In these cases, truncation is applied, but care must be taken not to cut off important parts of the context that contain the answer. An alternative strategy could involve splitting long contexts and feeding them to the model in multiple parts.\n",
    "\n",
    "Step 3: Model Setup\n",
    "A pre-trained BERT model is loaded from Hugging Face's transformers library. Specifically, BertForQuestionAnswering is used, which adds additional layers to BERT to handle start and end position predictions for answers. This model has already learned rich language representations from its pre-training phase, but it requires fine-tuning on the specific SQuAD dataset to adapt it to the QA task.\n",
    "\n",
    "Step 4: Training Process\n",
    "The model is fine-tuned using AdamW (Adam optimizer with weight decay), a common optimizer for transformer-based models. Training involves passing tokenized questions and contexts into the model, where the model predicts two values for each token: the probability that the token is the start of the answer and the probability that it is the end of the answer.\n",
    "\n",
    "During training, the key challenge is ensuring that the model learns effectively from the data without overfitting. This is mitigated by splitting the dataset into training and validation sets. The training set is used to update the model weights, while the validation set is used to check the model's performance on unseen data after each epoch. The loss function used is cross-entropy, which calculates the difference between the predicted start and end positions and the true start and end positions of the answers.\n",
    "\n",
    "Automatic Mixed Precision (AMP) is applied to speed up training, allowing operations to be performed using lower precision (FP16) where possible, reducing memory usage without sacrificing much accuracy. Gradient accumulation is also used to simulate a larger batch size by accumulating gradients over several smaller batches before updating the weights, which is especially helpful on GPUs with limited memory.\n",
    "\n",
    "Step 5: Challenges in Fine-Tuning\n",
    "The main challenges in fine-tuning BERT for QA are:\n",
    "\n",
    "Memory Usage: BERT models are large and require significant computational resources, especially when working with long contexts. This project addresses this by using mixed precision training and gradient accumulation, which reduce memory requirements.\n",
    "\n",
    "Answer Position Mapping: Accurately mapping the answer's character positions to the correct token positions is crucial. If the start and end positions are not correctly aligned after tokenization, the model will be confused during training. Tokenization can sometimes split words, so adjustments need to be made to handle these shifts.\n",
    "\n",
    "Overfitting: As BERT has a large capacity, it can overfit to the training data if not regularized properly. Careful use of the validation set, a moderate learning rate (3e-5), and a small number of epochs (typically 2-4) help mitigate overfitting. Weight decay (via AdamW) also helps in regularizing the model.\n",
    "\n",
    "Long Inputs: Some contexts exceed the token limit for BERT, so handling long sequences by truncating or splitting them into manageable parts is important. This needs to be done carefully to avoid removing parts of the context that contain the answer.\n",
    "\n",
    "Step 6: Validation and Evaluation\n",
    "The model is evaluated on a validation set using a similar procedure to the training process, but without updating the model's weights (i.e., in evaluation mode). For each context-question pair, the model predicts the most likely start and end positions of the answer, and the loss is calculated similarly to the training phase.\n",
    "\n",
    "The performance of the model can be measured using metrics like Exact Match (EM), which checks if the predicted answer exactly matches the true answer, and F1-score, which measures the overlap between the predicted and true answers. These metrics give an indication of how well the model generalizes to unseen data.\n",
    "\n",
    "Step 7: Saving the Model\n",
    "Once training is complete, the model's state dictionary (the model’s learned parameters) is saved to a file (finetunemodel.pt). This allows the fine-tuned model to be reloaded later for inference or further fine-tuning without retraining from scratch. The tokenizer is also saved, as it is essential to use the same tokenization strategy during inference as was used during training.\n",
    "\n",
    "Step 8: Inference\n",
    "During inference, a user provides a context and a question. The context-question pair is tokenized and passed through the fine-tuned model, which predicts the start and end positions of the answer. These token positions are then converted back into the original words using the tokenizer, giving the final answer.\n",
    "\n",
    "Conclusion\n",
    "Building and fine-tuning a BERT-based QA model is a complex process that involves handling large datasets, managing tokenization challenges, and carefully tuning hyperparameters for effective learning. While BERT provides a strong foundation with its pre-trained language representations, fine-tuning it for a specific task like QA requires careful handling of data preprocessing, model setup, and training mechanics to achieve high performance. With the model fine-tuned and validated, it can be deployed for real-world QA tasks or shared with others through platforms like Hugging Face's Model Hub."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
